\section{Case Studies}
\label{sct:case-studies}

In this section we present selected use-cases for compile-time views that, at the
same time, demonstrate step-by-step the mechanics behind \tool. We start by inlining a simple function with staging
(\sct{sct:inlining}), then do the canonical staging  example of the integer power function
(\sct{sct:recursion}), then we demonstrate how variable argument functions can
be  desugared into the core functionality (\sct{sct:varargs}). Finally, we
demonstrate how the abstraction overhead of the \code{dot} function and all
associated type-class related abstraction an be removed (\sct{sct:dot-product}).

\subsection{Inlining Expressed Through Staging}
\label{sct:inlining}

Function inlining can be expressed as staged computation~\cite{monnier2003inlining}.
 Inlining is achieved when a statically known function body is applied with symbolic
 arguments. In \tool we use the \code{ct} annotation on functions and methods to achieve inlining:\begin{lstparagraph}
@ct def zero[T](implicit num: Numeric[T]) = num.zero

zero[Double]
  $\hookrightarrow$ num.zero
\end{lstparagraph}


\subsection{Recursion}
\label{sct:recursion}

The canonical example in staging literature is partial evaluation of the power function
 where exponent is an integer:
\begin{lstparagraph}
def pow(base: Double, exp: Int): Double =
  if (exp == 0) 1 else base * pow(base, exp - 1)
\end{lstparagraph} When the exponent (\code{exp}) is statically known this function can be partially
evaluated into \code{exp} multiplications of the \code{base} argument, significantly
improving performance~\cite{calcagno2003implementing}.

With compile-time views making \code{pow} partially evaluated requires adding only one annotation:

\begin{lstparagraph}
def pow(base: Double, exp: Int@ct): Double =
  if (exp == 0) 1 else base * pow(base, exp - 1)
\end{lstparagraph}

% TODO cite infinite recursion
To satisfy cross-stage persistence (\sct{sct:wf-ctv}) the \code{pow} must be \code{@ct}.
This annotation is automatically added by \tool as described in \sct{sct:implicits}. In the example,
 the \code{ct} annotation on \code{exp} requires that the function must be called with
 a compile-time view of \code{Int}. \tool ensures that the definiton of the \code{pow} function
 does not cause infinite recursion at compile-time by invoking the power function
 only when the value of the \code{ct} arguments is known.

 The application of the function \code{pow} with a constant
 exponent produces:

\begin{lstparagraph}
pow(base, 4)
  $\hookrightarrow$ base * base * base * base * 1
\end{lstparagraph}

Constant 4 is promoted to \code{ct} by the implicit conversions (\sct{sct:implicits}).

\subsection{Variable Argument Functions}
\label{sct:varargs}

% Variable argument functions
Variable argument functions appear in widely used languages like Java, C\#, and Scala.
 Such arguments are typically passed in the function body inside of the data structure
 (\eg \code{Seq[T]} in Scala). When applied with variable arguments the size of the
 data-structure is statically known and all operations on them can be partially
 evaluated. However, sometimes, the function is called with arguments of dynamic size.
 For example, function \code{min} that accepts multiple integers\begin{lstparagraph}
def min(vs: Int*): Int = vs.tail.foldLeft(vs.head) {
  (min, el) => if (el < min) el else min
}
\end{lstparagraph}can be called either with statically known arguments
 (\eg, \code{min(1,2)}) or with dynamic arguments:\begin{lstparagraph}
val values: Seq[Int] = ... // dynamic value
min(values: _*)
\end{lstparagraph}

Ideally, we would be able to achieve partial evaluation if the arguments are of statically
known size and avoid partial evaluation in case of dynamic arguments. To this end we translate
the method \code{min} into a partially evaluated version and a dynamic version. The call to these
methods is dispatched, at compile-time, by the \code{min} method which checks if
arguments are statically known. Desugaring of \code{min} is shown in \figref{fig:min}.

\begin{figure}
\begin{listing}
def min(vs: Int*): Int = macro
  if (isVarargs(vs)) q"min_CT(vs)"
  else q"min_D(vs)"

def min_CT(vs: Seq[Int]@ct): Int =
  vs.tail.foldLeft(vs.head) { (min, el) =>
    if (el < min) el else min
  }

def min_D(vs: Seq[Int]): Int =
  vs.tail.foldLeft(vs.head) {
    (min, el) => if (el < min) el else min
  }
\end{listing}
\caption{Function \code{min} is desugared into a \code{min} macro that based on the
binding time of the arguments dispatches to the partially evaluated version (\code{min_CT})
for statically known varargs or to the original min function for dynamic arguments \code{min_D}.}
\label{fig:min}
\end{figure}

\subsection{Removing Abstraction Overhead of Type-Classes}
\label{sct:type-classes-removal}

Type-classes are omnipresent in everyday programming as they allow abstraction over
 generic parameters (\eg, \code{Numeric} abstracts over numeric values). Unfortunately,
 type-classes introduce \emph{dynamic dispatch} on every call~\cite{rompf_optimizing_2013} and,
 thus, impose a performance penalty. Type-classes are in most of the cases statically known. Here
 we show how with \tool we can remove all abstraction overheads of type classes.

In Scala, type classes are implemented with objects and implicit parameters~\cite{oliveira_type_2010}.
In \figref{fig:numeric}, we define a \code{trait Numeric} serves as an interface for
all numeric types. Then we define a concrete implementation of \code{Numeric} for
type \code{Double} (\code{DoubleNumeric}). The \code{DoubleNumeric} is than passed
as an implicit argument \code{dnum} to all methods that use it (\eg, \code{zero}).

\begin{figure}
\begin{listing}
object Numeric {
  implicit def dnum: Numeric[Double]@ct =
    ct(DoubleNumeric)
  def zero[T](implicit num: Numeric[T]@ct): T =
    num.zero
}

trait Numeric[T] {
  def plus(x: T, y: T): T
  def times(x: T, y: T): T
  def zero: T
}

object DoubleNumeric extends Numeric[Double] {
  def plus(x: Double, y: Double): Double = x + y
  def times(x: Double, y: Double): Double = x * y
  def zero: Double = 0.0
}
\end{listing}
\caption{\label{fig:numeric} Removing abstraction overheads of type classes.}
\end{figure}

When \code{zero} is applied first the implicit argument (\code{dnum}) gets
inlined due to the \code{ct} annotation of the return type, then the function \code{zero} gets
inlined. Since \code{dnum} returns a compile-time view of \code{DoubleNumerc}
the method \code{zero} on \code{dnum} is evaluated at compile time. The constant \code{0.0} is
promoted to \code{ct} since \code{DoubleNumeric} is a compile time view. Finally the \code{ct(0.0)} result
is coerced to a dynamic value by the signature of \code{Numeric.zero}. The
compile-time execution is shown in the following snippet

\begin{lstparagraph}
Numeric.zero[Double]
  $\hookrightarrow$ Numeric.zero[Double](DoubleNumeric)
  $\hookrightarrow$ ct(DoubleNumeric).zero
  $\hookrightarrow$ (ct(0.0): Double)
  $\hookrightarrow$ 0.0
\end{lstparagraph}

\subsection{Inner Product of Vectors}
\label{sct:dot-product}

Here we demonstrate how the introductory example (\sct{sct:introduction}) is
partially evaluated through staging. We start with the desugared \code{dot}
function~(\ie, all implicit operations are shown):

\begin{lstparagraph}
 def dot[V](v1: Vector[V]@ct, v2: Vector[V]@ct)
  (implicit num: Numeric[V]@ct): V =
  (v1 zip v2).foldLeft(zero[V](num)) {
    case (prod, (cl, cr)) => prod + cl * cr
  }
\end{lstparagraph}

Function \code{dot} is generic in the type of vector elements. This will reflect
upon the staging annotations as well (\code{ct} and \code{static}). When we apply the
\code{dot} function with static arguments we will get the vector with static elements back:

\begin{lstparagraph}
dot[Double@static](
  ct(Vector)(2.0, 4.0), ct(Vector)(1.0, 10.0))(
  Numeric.dnum)
$\hookrightarrow$
  (ct(Vector)(2.0, 4.0) zip ct(Vector)(1.0, 10.0))
    .foldLeft(ct(0.0)) {
      case (prod, (cl, cr)) => prod + cl * cr
    }
$\hookrightarrow$ (2.0 * 1.0 + 4.0 * 10.0): Double@static
\end{lstparagraph}

When \code{dot} is evaluated with the \code{ct} elements the last step will further
execute to a single compile-time value that can further be used in compile-time computations:
\begin{lstparagraph}
dot[Double@ct](
  ct(Vector)(ct(2.0), ct(4.0)),
  ct(Vector)(ct(1.0), ct(10.0)))(Numeric.dnum)
$\hookrightarrow$ ct(2.0) * ct(1.0) + ct(4.0) * ct(10.0)
$\hookrightarrow$ 42.0: Double@ct
\end{lstparagraph}
