\section{Case Studies}
\label{sct:case-studies}

In this section we present selected use-cases for compile-time views that at the
same  time demonstrate step-by-step the mechanics behind \tool and the
interesting applications.  We start by inlining a simple function with staging
(\sct{sct:inlining}), then do the canonical staging  example of the power function
(\sct{sct:recursion}), then we demonstrate how variable argument functions can
be  desugared into the core functionality (\sct{sct:varargs}). Finally, we
demonstrate how the abstraction overhead of the \code{dot} function and all
associated type-class related abstraction an be removed (\sct{sct:dot-product}).

\subsection{Inlining Expressed Through Staging}
\label{sct:inlining}




\subsection{Recursion}
\label{sct:recursion}

The canonical example in staging literature is partial evaluation of the power function
 where exponent is integer:
\begin{lstparagraph}
def pow(base: Double, exp: Int): Double =
  if (exp == 0) 1 else base * pow(base, exp)
\end{lstparagraph} When the exponent (\code{exp}) is statically known this function can be partially
evaluated into \code{exp} multiplications of the \code{base} argument, significantly
improving performance~\cite{}.

With compile-time views making\code{pow} partially evaluated requires adding two annotations:

\begin{lstparagraph}
@inline def pow(base: Double, exp: Int@ct): Double =
  if (exp == 0) 1 else base * pow(base, exp)
\end{lstparagraph}

\code{@inline} denotes that the \code{pow} function it self must be executed at
 compile time and \code{@ct} requires that the \code{exp} argument is a compile-time
 view of \code{Int}. The application of the function \code{pow} with a constant
 exponent will produce:

\begin{lstparagraph}
pow(base, 4)
  $\hookrightarrow$ base * base * base * base * 1
\end{lstparagraph}

Here, in the function application, constant 4 is promoted to \code{ct} by the implicit
conversions (\sct{sct:implicits}).

\subsection{Variable Argument Functions}
\label{sct:varargs}

% Variable argument functions
Variable argument functions appear in widely used languages like Java, C\#, and Scala.
 Such arguments are typically passed in the function body inside of the data structure
 (\eg \code{Seq[T]} in Scala). When applied with variable arguments the size of the
 data-structure is statically known and all operations on them can be partially
 evaluated. However, sometimes, the function is called with arguments of dynamic size.
 For example, function \code{min} that accepts multiple integers\begin{lstparagraph}
def min(vs: Int*): Int = vs.tail.foldLeft(vs.head) {
  (min, el) => if (el < min) el else min
}
\end{lstparagraph}can be called either with statically known arguments
 (\eg, \code{min(1,2)}) or with dynamic arguments:\begin{lstparagraph}
val values: Seq[Int] = ... // dynamic value
min(values: _*)
\end{lstparagraph}

Ideally, we would be able to achieve partial evaluation if the arguments are of statically
known size and avoid partial evaluation in case of dynamic arguments. To this end we translate
the method \code{min} into a partially evaluated version and a dynamic version. The call to these
methods is dispatched, at compile-time, by the \code{min} method which checks if
arguments are statically known. Desugaring of \code{min} is shown in \figref{fig:min}.

\begin{figure}
\begin{listing}
def min(vs: Int*): Int = macro
  if (isVarargs(vs)) q"min_CT(vs)"
  else q"min_D(vs)"

def min_CT(vs: Seq[Int] @ct): Int =
  vs.tail.foldLeft(vs.head) { (min, el) =>
  if (el < min) el else min
}
def min_D(vs: Seq[Int]): Int =
  vs.tail.foldLeft(vs.head) {
    (min, el) => if (el < min) el else min
  }
\end{listing}
\caption{Function \code{min} is desugared into a \code{min} macro that based on the
binding time of the arguments dispatches to the partially evaluated version (\code{min_CT})
for statically known varargs or to the original min function for dynamic arguments \code{min_D}.}
\label{fig:min}
\end{figure}

\subsection{Removing Abstraction Overhead of Type-Classes}
\label{sct:type-classes-removal}
\todo{cite}
Type-classes are omnipresent in everyday programming as they provide allow abstraction over
 generic parameters (\eg Numeric abstracts over numeric values). Unfortunately,
 type-classes are a source of abstraction overheads during execution\todo{cite}. Type-classes
 are in most of the cases statically known. Ideally, we would be able to
 deterministically remove abstraction overheads of type classes.

\begin{figure}
\begin{listing}
object Numeric {
  @inline implicit def dnum: Numeric[Double]@ct =
    ct(DoubleNum)
  @inline def zero[T](implicit num: Numeric[T]@ct): T =
    num.zero
}

trait Numeric[T] {
  def plus(x: T, y: T): T
  def times(x: T, y: T): T
  def zero: T
}

object DoubleNumeric extends Numeric[Double] {
  def plus(x: Double, y: Double): Double = x + y
  def times(x: Double, y: Double): Double = x * y
  def zero: Double = 0.0
}
\end{listing}
\caption{\label{fig:numeric} Function for computing the non-negative power of a real number.}
\end{figure}


\subsection{Dot Product}
\label{sct:dot-product}
\begin{itemize}
  \item Explain the removal of type classes together with inline. Explain how type classes
  are @i? and how they will completely evaluate if they are passed a static value.
  \item Comparison to other approaches.
\end{itemize}

