\section{Case Studies}
\label{sct:case-studies}

In this section we present selected use-cases for compile-time views that at the
same  time demonstrate step-by-step the mechanics behind \tool and the
interesting applications.  We start by inlining a simple function with staging
(\sct{sct:inlining}), then do the canonical staging  example of the power function
(\sct{sct:recursion}), then we demonstrate how variable argument functions can
be  desugared into the core functionality (\sct{sct:varargs}). Finally, we
demonstrate how the abstraction overhead of the \code{dot} function and all
associated type-class related abstraction an be removed (\sct{sct:dot-product}).
For formal partial evaluation rules refer \cf \sct{fig:partial-evaluation}.

\subsection{Inlining Expressed Through Staging}
\label{sct:inlining}

Function inlining can be expressed as staged computation~\cite{monnier2003inlining}.
 Inlining is achieved when a statically known function body is applied with symbolic
 arguments. In \tool we use the \code{inline} annotation on functions and methods to achieve inlining:\begin{lstparagraph}
@inline def zero(implicit num: Numeric) = num.zero
zero[Double]
  $\hookrightarrow$ num.zero
\end{lstparagraph}


\subsection{Recursion}
\label{sct:recursion}

The canonical example in staging literature is partial evaluation of the power function
 where exponent is an integer:
\begin{lstparagraph}
def pow(base: Double, exp: Int): Double =
  if (exp == 0) 1 else base * pow(base, exp)
\end{lstparagraph} When the exponent (\code{exp}) is statically known this function can be partially
evaluated into \code{exp} multiplications of the \code{base} argument, significantly
improving performance~\cite{calcagno2003implementing}.

With compile-time views making\code{pow} partially evaluated requires adding two annotations:

\begin{lstparagraph}
@inline def pow(base: Double, exp: Int@ct): Double =
  if (exp == 0) 1 else base * pow(base, exp)
\end{lstparagraph}

\code{@inline} denotes that the \code{pow} function it self must be executed at
 compile time and \code{@ct} requires that the \code{exp} argument is a compile-time
 view of \code{Int}. The application of the function \code{pow} with a constant
 exponent will produce:

\begin{lstparagraph}
pow(base, 4)
  $\hookrightarrow$ base * base * base * base * 1
\end{lstparagraph}

Here, in the function application, constant 4 is promoted to \code{ct} by the implicit
conversions (\sct{sct:implicits}).

\subsection{Variable Argument Functions}
\label{sct:varargs}

% Variable argument functions
Variable argument functions appear in widely used languages like Java, C\#, and Scala.
 Such arguments are typically passed in the function body inside of the data structure
 (\eg \code{Seq[T]} in Scala). When applied with variable arguments the size of the
 data-structure is statically known and all operations on them can be partially
 evaluated. However, sometimes, the function is called with arguments of dynamic size.
 For example, function \code{min} that accepts multiple integers\begin{lstparagraph}
def min(vs: Int*): Int = vs.tail.foldLeft(vs.head) {
  (min, el) => if (el < min) el else min
}
\end{lstparagraph}can be called either with statically known arguments
 (\eg, \code{min(1,2)}) or with dynamic arguments:\begin{lstparagraph}
val values: Seq[Int] = ... // dynamic value
min(values: _*)
\end{lstparagraph}

Ideally, we would be able to achieve partial evaluation if the arguments are of statically
known size and avoid partial evaluation in case of dynamic arguments. To this end we translate
the method \code{min} into a partially evaluated version and a dynamic version. The call to these
methods is dispatched, at compile-time, by the \code{min} method which checks if
arguments are statically known. Desugaring of \code{min} is shown in \figref{fig:min}.

\begin{figure}
\begin{listing}
def min(vs: Int*): Int = macro
  if (isVarargs(vs)) q"min_CT(vs)"
  else q"min_D(vs)"

def min_CT(vs: Seq[Int] @ct): Int =
  vs.tail.foldLeft(vs.head) { (min, el) =>
  if (el < min) el else min
}
def min_D(vs: Seq[Int]): Int =
  vs.tail.foldLeft(vs.head) {
    (min, el) => if (el < min) el else min
  }
\end{listing}
\caption{Function \code{min} is desugared into a \code{min} macro that based on the
binding time of the arguments dispatches to the partially evaluated version (\code{min_CT})
for statically known varargs or to the original min function for dynamic arguments \code{min_D}.}
\label{fig:min}
\end{figure}

\subsection{Removing Abstraction Overhead of Type-Classes}
\label{sct:type-classes-removal}

Type-classes are omnipresent in everyday programming as they provide allow abstraction over
 generic parameters (\eg, \code{Numeric} abstracts over numeric values). Unfortunately,
 type-classes introduce \emph{dynamic dispatch} on every call~\cite{rompf_optimizing_2013} and are,
 thus, impose a performance penalty. Type-classes are in most of the cases statically known. Here
 we show how with \tool we can remove all abstraction overheads of type classes.

In Scala, type classes are implemented with objects and implicit parameters~\cite{oliveira_type_2010}.
In \figref{fig:numeric}, we define a \code{trait Numeric} serves as an interface for
all numeric types. Then we define a concrete implementation of \code{Numeric} for
type \code{Double} (\code{DoubleNumeric}). The \code{DoubleNumeric} is than passed
as an implicit argument \code{dnum} to all methods that use it (\eg, \code{zero}).

\begin{figure}
\begin{listing}
object Numeric {
  @inline implicit def dnum: Numeric[Double]@ct =
    ct(DoubleNumeric)
  @inline def zero[T](implicit num: Numeric[T]@ct): T =
    num.zero
}

trait Numeric[T] {
  def plus(x: T, y: T): T
  def times(x: T, y: T): T
  def zero: T
}

object DoubleNumeric extends Numeric[Double] {
  def plus(x: Double, y: Double): Double = x + y
  def times(x: Double, y: Double): Double = x * y
  def zero: Double = 0.0
}
\end{listing}
\caption{\label{fig:numeric} Removing abstraction overheads of type classes.}
\end{figure}

When \code{zero} is applied first the implicit argument (\code{dnum}) gets inlined due to the \code{inline} annotation,
then the function zero gets inlined. Since \code{dnum} returns the compile-time view
of \code{DoubleNumerc} the method \code{zero} is evaluated at compile time. The constant
\code{0.0} is promoted to \code{ct} since \code{DoubleNumeric} is a compile time view (formally defined in \sct{sct:compile-views}).
Finally the \code{ct(0.0)} result is coerced to a dynamic value by the signature of \code{Numeric.zero}. The
compile-time execution is shown in the following snippet\begin{lstparagraph}
Numeric.zero[Double]
  $\hookrightarrow$ Numeric.zero[Double](DoubleNumeric)
  $\hookrightarrow$ ct(DoubleNumeric).zero
  $\hookrightarrow$ (ct(0.0): Double)
  $\hookrightarrow$ 0.0
\end{lstparagraph}

\subsection{Inner Product of Vectors}
\label{sct:dot-product}

